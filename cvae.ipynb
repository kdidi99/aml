{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdidi99/ml_advanced_hd/blob/main/cvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UteRPZKLC86"
      },
      "source": [
        "# Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD03NRC6LC89"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwEc1jo_LC8-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "    \n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KoxSUeQLC9B"
      },
      "outputs": [],
      "source": [
        "#Converts label idx (n labels total) into one-hot encoding\n",
        "def idx2onehot(idx, n):\n",
        "\n",
        "    assert torch.max(idx).item() < n\n",
        "    if idx.dim() == 1:\n",
        "        idx = idx.unsqueeze(1)\n",
        "\n",
        "    onehot = torch.zeros(idx.size(0), n)\n",
        "    onehot.scatter_(1, idx, 1)\n",
        "\n",
        "    return onehot\n",
        "\n",
        "#example:\n",
        "idx2onehot(3*torch.ones(7, dtype=torch.long),n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY0h_sqOLC9B"
      },
      "source": [
        "## Basics/Repetition  of VAE\n",
        "\n",
        "In the lecture we have shown that $$\\log p^*(x^{(i)}) \\ge  -  D_{KL}[p_E(z \\mid x^{(i)}) \\| p(z)] +\\mathbb{E}_{z\\sim p_E(z \\mid x^{(i)})} [\\log p_D (x^{(i)} \\mid z)] = -\\mathcal{L}(D, E, x^{(i)}),$$\n",
        "where $x^{(i)}\\in\\mathbb{R}^D$ is the $i$-th training instance (since the pixel values of MNIST images are in the range 0...1, we even have $x^{(i)}\\in[0,1]^D$ in this case). The LHS is the logarithm of the true data distribution, and the RHS is termed the \"evicence lower bound\" (ELBO).\n",
        "\n",
        "We call $p_E(z \\mid x)$ the encoder and $p_D( x \\mid z)$ the decoder. Both will be represented by neural networks. Our goal is to approximate $p^*(x)$ as well as possible by maximizing the ELBO or equivalently minimizing its negation. Specifically, we minimize $\\mathcal{L}(D, E, x^{(i)})$ with respect to the parameters of the decoder network $D$ and the encoder network $E$ via gradient descent over all training instances $i$. \n",
        "\n",
        "In order to estimate the negative ELBO, we approximate the expectation w.r.t. $z$ by its average over $L$ instances:\n",
        "\\begin{align} \\hat{\\mathcal{L}}(D, E, x^{(i)}) = D_{KL}\\left[p_E(z \\mid x^{(i)}) \\| p(z)\\right]+ \\frac{1}{L} \\sum_{l=1}^L \\left(-\\log p_D(x^{(i)} \\mid z^{(i,l)})\\right)\\end{align} \n",
        "where $z^{(i,l)} \\sim p_E(z \\mid x^{(i)}) $. By construction of a VAE, $p_E(z \\mid x^{(i)})$ is a Gaussian distribution whose mean $\\mu^{(i)}=\\mu_E(x^{(i)})$ and standard deviation $\\sigma^{(i)}=\\sigma_E(x^{(i)})$ are computed by the encoder network. For fixed $x^{(i)}$, we can draw samples $z^{(i,l)}$ from this code distribution by means of the reparametrization trick: \n",
        "$$z^{(i,l)}\\sim \\mathcal{N}\\big(\\mu^{(i)}, \\text{diag}(\\sigma^{(i)})^2\\big) \\Leftrightarrow  z^{(i,l)} = \\mu^{(i)} + \\epsilon_l \\cdot \\sigma^{(i)}$$ \n",
        "with $\\epsilon_l\\sim\\mathcal{N}(0, \\mathbb{I})$. Note that $\\mu^{(i)}$, $\\sigma^{(i)}$, and $\\epsilon_l$ are vectors of length equal to the dimension $J$ of the latent space, and $\\epsilon_l \\cdot \\sigma^{(i)}$ is element-wise multiplication. In practice, $L=1$ is usually sufficient so that the average over index $l$ becomes trivial.\n",
        "\n",
        "Furthermore, we assume that the latent prior is a standard normal distribution, i.e. $p(z) = \\mathcal{N}(0, \\mathbb{I})$. The KL-term for the two multivariate normal distributions can then be computed analytically:\n",
        "$$ D_{KL}\\left[p_E(z \\mid x^{(i)}) \\| p(z)\\right] = \\frac{1}{2} \\sum_{j=1}^J \\left((\\mu_j^{(i)})^2 + (\\sigma_j^{(i)})^2 - 2 \\log(\\sigma_j^{(i)}) - 1\\right) $$\n",
        "\n",
        "Likewise, we consider $p_D(x \\mid z)$ as a Gaussian distribution with mean $\\mu_D(z)$ and fixed covariance matrix $\\sigma_G^2\\cdot \\mathbb{I}$ (i.e. $\\sigma_G$ is the fixed noise standard deviation):\n",
        "$$ p_G(x \\mid z) = \\mathcal{N}\\big(\\mu_D(z), \\sigma_G^2\\cdot\\mathbb{I}\\big)$$\n",
        "To ensure that $\\mu_D(z) \\in [0,1]^D$ holds for reconstructed images (without noise), the decoder's output layer should use the sigmoid activation function. The second term in the negated ELBO (the negative log-likelihood) now reduces to the squared loss:\n",
        "$$-\\log p_D(x^{(i)} \\mid z^{(i,l)}) = \\frac{||x^{(i)} - \\mu_D(z^{(i,l)})||^2_2}{2 \\sigma_G^2}  + \\text{const.}$$\n",
        "The additive constant has no influence on the training optimimum and can be dropped. $\\sigma_G$ can be used as a hyperparameter to balance the two loss terms.\n",
        "\n",
        "For a batch of samples $X = (x^{(1)}, \\dots, x^{(M)})$, we finally get the negated ELBO as:\n",
        "\\begin{align} -ELBO = \\sum_{i=1}^M \\Big[&\\frac{1}{2} \\sum_{j=1}^J \\left((\\mu_j^{(i)})^2 + (\\sigma_j^{(i)})^2 - 2 \\log(\\sigma_j^{(i)}) - 1\\right) \\\\+& \\frac{1}{L} \\sum_{l=1}^L \\sum_{j=1}^D \\frac{(x^{(i)}_j - \\mu_D(z^{(i,l)})_j)^2}{2\\sigma_G^2}\\Big]\\end{align}\n",
        "Training is performed by gradient descent on this loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8qgo9DeLC9E"
      },
      "source": [
        "## Task 1: Implementation of VAE and CVAE\n",
        "\n",
        "Complete the code below. The CVAE class consists of three parts\n",
        "* The Encoder class that implements $p_E (z \\mid x, y)$,\n",
        "* The Decoder class which implements $p_D (x \\mid z, y)$ and\n",
        "* The actual CVAE class that combines both encoder and decoder.\n",
        "\n",
        "The conditioning variable $y$ holds the labels, e.g. 0...9 for MNIST digits. It is added as an additional network input, i.e. the encoder computes $\\mu_E(x^{(i)}, y^{(i)})$ and $\\sigma_E(x^{(i)}, y^{(i)})$. The decoder produces the reconstruction `recon_x`=$\\mu_D(z^{(i)}, y^{(i)})$, where $z^{(i)}$ is sampled using the reparametrization trick explained above. \n",
        "\n",
        "Implement all three classes. The arguments and outputs for each method are given in the docstrings. Make sure the CVAE class implements both the conditional VAE (CVAE) and the plain VAE, where the latter is obtained if the number of labels is just 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUAtsS0-LC9F"
      },
      "outputs": [],
      "source": [
        "class CVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, inp_dim, encoder_layer_sizes, decoder_layer_sizes, latent_dim, num_labels=10, conditional=False):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            inp_dim (int): dimension of input,\n",
        "            encoder_layer_sizes (list[int]): list of the sizes of the encoder layers,\n",
        "            decoder_layer_sizes (list[int]): list of the sizes of the decoder layers,\n",
        "            latent_dim (int): dimension of latent space/bottleneck,\n",
        "            num_labels (int): amount of labels (important for conditional VAE),,\n",
        "            conditional (bool): True if CVAE, else False\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        super(CVAE, self).__init__()\n",
        "        \n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_labels = num_labels\n",
        "        \n",
        "        self.encoder = Encoder(encoder_layer_sizes, latent_dim, num_labels, conditional)\n",
        "        self.decoder = Decoder(decoder_layer_sizes, latent_dim, num_labels, conditional)\n",
        "        \n",
        "    def forward(self, x, c=None):\n",
        "        \"\"\"\n",
        "        Forward Process of whole VAE/CVAE. \n",
        "        Arguments:\n",
        "            x: tensor of dimension (batch_size, 1, 28, 28) or (batch_size, 28*28)\n",
        "            c: None or tensor of dimension (batch_size, 1)\n",
        "        Output: recon_x, means, log_var\n",
        "            recon_x: see explanation on second part of estimator above,\n",
        "            means: output of encoder,\n",
        "            log_var: output of encoder (logarithm of variance)\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        x = x.view(-1,784)\n",
        "        \n",
        "        ################################\n",
        "        # TODO: YOUR CODE STARTS BELOW #\n",
        "        ################################\n",
        "        \n",
        "        \n",
        "        ################################\n",
        "        #     YOUR CODE ENDS HERE      #\n",
        "        ################################\n",
        "\n",
        "        return recon_x, means, log_var\n",
        "        \n",
        "    def sampling(self, n=2, c=None):\n",
        "        \"\"\"\n",
        "        Generates new samples by feeding a random latent vector to the decoder.\n",
        "        Arguments:\n",
        "            n (int): amount of samples \n",
        "            c      : None or tensor of dimension (batch_size, 1) (labels to condition on)\n",
        "        Output:\n",
        "            x_sampled: n randomly sampled elements of the output distribution\n",
        "        \"\"\"\n",
        "        ################################\n",
        "        # TODO: YOUR CODE STARTS BELOW #\n",
        "        ################################\n",
        "\n",
        "        \n",
        "        ################################\n",
        "        #     YOUR CODE ENDS HERE      #\n",
        "        ################################\n",
        "        return x_sampled \n",
        "    \n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_sizes, latent_dim, num_labels, conditional=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            layer_sizes (list[int]): list of sizes of layers of the encoder,\n",
        "            latent_dim (int): dimension of latent space, i.e. dimension out output of the encoder,\n",
        "            num_labels (int): amount of labels,\n",
        "            conditional (bool): True if CVAE and False if VAE\n",
        "        \"\"\"\n",
        "        \n",
        "        ################################\n",
        "        # TODO: YOUR CODE STARTS BELOW #\n",
        "        ################################\n",
        "        #hint: if conditional the input layersize needs to be increased for the additional one-hot vector input\n",
        "                \n",
        "\n",
        "        ################################\n",
        "        #     YOUR CODE ENDS HERE      #\n",
        "        ################################\n",
        "    \n",
        "    def forward(self, x, c=None):  \n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: tensor of dimension (batch_size, 1, 28, 28) or (batch_size, 28*28)\n",
        "            c: None or tensor of dimension (batch_size, 1)\n",
        "        Output:\n",
        "            means: tensor of dimension (batch_size, latent_dim),\n",
        "            log_var: tensor of dimension (batch_size, latent_dim)\n",
        "        \"\"\"\n",
        "        ################################\n",
        "        # TODO: YOUR CODE STARTS BELOW #\n",
        "        ################################\n",
        "        #Hint: convert condition c into one-hot encoding\n",
        "        \n",
        "        \n",
        "        ################################\n",
        "        #     YOUR CODE ENDS HERE      #\n",
        "        ################################        \n",
        "        return means, log_vars\n",
        "    \n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_sizes, latent_dim, num_labels, conditional=False):     \n",
        "        super(Decoder, self).__init__()\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            layer_sizes (list[int]): list of sizes of layers of the decoder,\n",
        "            latent_dim (int): dimension of latent space, i.e. dimension out input of the decoder,\n",
        "            num_labels (int): amount of labels,\n",
        "            conditional (bool): True if CVAE and False if VAE\n",
        "        Output:\n",
        "            x: Parameters of gaussian distribution; only mu (see above)\n",
        "        \"\"\"\n",
        "\n",
        "        ################################\n",
        "        # TODO: YOUR CODE STARTS BELOW #\n",
        "        ################################\n",
        "        #hint: if conditional is True, the input layersize needs to be increased for the additional one-hot vector input\n",
        "\n",
        "        \n",
        "        ################################\n",
        "        #     YOUR CODE ENDS HERE      #\n",
        "        ################################\n",
        "            \n",
        "    def forward(self, z, c=None):\n",
        "        \"\"\"\n",
        "        Argumetns:\n",
        "            z: tensor of dimension (batch_size, latent_dim)\n",
        "            c: None or tensor of dimension (batch_size, 1)\n",
        "        Outputs:\n",
        "            x: mu of gaussian distribution (reconstructed image from latent code z)\n",
        "        \"\"\"\n",
        "        ################################\n",
        "        # TODO: YOUR CODE STARTS BELOW #\n",
        "        ################################\n",
        "        #Hint: convert condition c into one-hot encoding\n",
        "\n",
        "        \n",
        "        ################################\n",
        "        #     YOUR CODE ENDS HERE      #\n",
        "        ################################\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmMpcnNVLC9G"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "We want to minimize the negated ELBO loss:\n",
        "$$\\hat{\\mathcal{L}}(D, E, x^{(i)}) = D_{KL}\\left[p_E(z \\mid x^{(i)}) \\| p(z)\\right]+ \\frac{1}{L} \\sum_{l=1}^L \\left(-\\log p_D(x^{(i)} \\mid z^{(i,l)})\\right)$$\n",
        "where $L=1$. The `loss_function` should implement this estimator, expanding the two terms as explained above.\n",
        "* Implement the loss function\n",
        "* Comment/explain how your code arises from the formula above.\n",
        "\n",
        "Hint: Use the following choice of hyperparameter $\\sigma_G = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIGax3yjLC9H"
      },
      "outputs": [],
      "source": [
        "# Implement the Loss function for the VAE/CVAE\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        recon_x: reconstruced input\n",
        "        x: input,\n",
        "        mu, log_var: parameters of posterior (distribution of z given x)\n",
        "    \"\"\"\n",
        "    ################################\n",
        "    # TODO: YOUR CODE STARTS BELOW #\n",
        "    ################################\n",
        "\n",
        "    \n",
        "    ################################\n",
        "    #     YOUR CODE ENDS HERE      #\n",
        "    ################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV4K2KSgLC9H"
      },
      "source": [
        "### Training of VAE\n",
        "Before we can do funny things with our VAE, we train it with a bottleneck size of two. If everything has been implemented correctly, you should obtain an VAE after a few epochs that is able to generate recognizable MNIST samples. \n",
        "\n",
        "The amount of layers as well as their dimensions do not have to be changed throughout this exercise. Better results might by achieved by searching for different hyperparameters.\n",
        "* Simply run the code to thrain the VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZPWn9_zLC9I"
      },
      "outputs": [],
      "source": [
        "encoder_layer_sizes = [784, 512, 256]\n",
        "decoder_layer_sizes = [256, 512, 784]\n",
        "\n",
        "latent_dim = 2 \n",
        "vae = CVAE(inp_dim=784, encoder_layer_sizes=encoder_layer_sizes, decoder_layer_sizes=decoder_layer_sizes, latent_dim=latent_dim)\n",
        "vae = vae.to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "# Training of the VAE\n",
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        x, y = data\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch,  mu, log_var = vae(x)\n",
        "        loss = loss_function(recon_batch,  x, mu, log_var)\n",
        "        \n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "    \n",
        "epochs = 15 \n",
        "for epoch in range(epochs):\n",
        "    train(epoch)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eJyfz25LC9I"
      },
      "source": [
        "### Sanity Check VAE\n",
        "Your model should be able to reproduce the input image, i.e. the output of the VAE should look similar to be input. \n",
        " \n",
        "* Run the code to check if your model worked\n",
        "* How are the reconstructions different from the original?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3uFj9nQLC9J"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(npimg, vmin=0, vmax=1, cmap='gray')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "_, x= next(enumerate(train_loader))\n",
        "samples = x[0].to(device)[30:]\n",
        "samples_rec,   _, _ = vae(samples)\n",
        "samples_rec = samples_rec.detach().cpu().view(-1,28,28)\n",
        "print(\"Original             Reconstructed\")\n",
        "for i in range(0, 4):\n",
        "  plt.subplot(4,2,2*i+1)\n",
        "  imshow(samples[i,0])\n",
        "    \n",
        "  plt.subplot(4, 2, 2*i+2)\n",
        "  imshow(samples_rec[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKcWbyeCLC9J"
      },
      "source": [
        "Your model should be able to generate images that look similar to the samples of the MNIST dataset.\n",
        "* Run the code\n",
        "* Describe what you see"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5tx3Kk2LC9J"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 10):\n",
        "    plt.subplot(3,3,i)\n",
        "    sample = vae.sampling(n=1).detach().view(-1,28,28).cpu()\n",
        "    plt.tight_layout()\n",
        "    imshow(sample[0])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhCqG5YLC9K"
      },
      "source": [
        "### Training a second VAE with larger latent space dimension\n",
        "Note that the quality of the reconstruction will also depend on the dimension of the latent space. To explore this, train a second model called `vae2` and use it to reconstruct a few example digits to evaluate the change in quality. Comment on your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIvWh9wYLC9K"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# TODO #\n",
        "########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrXvEmWYLC9K"
      },
      "source": [
        "With increase latent space dimension ($dim(z) = 16$) the training loss is improved only slightly. Visibly, the reconstruction has slightly improved. The reconstruction of the 9 is more accurate. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHodRKtILC9L"
      },
      "source": [
        "### Training CVAE\n",
        "We optimize in the following the CVAE (simpy run the code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8R2_SJbLC9L"
      },
      "outputs": [],
      "source": [
        "encoder_layer_sizes = [784, 512, 256]\n",
        "decoder_layer_sizes = [256, 512, 784]\n",
        "latent_dim = 2\n",
        "cvae = CVAE(inp_dim=784, encoder_layer_sizes=encoder_layer_sizes, decoder_layer_sizes=decoder_layer_sizes, latent_dim=latent_dim, conditional=True )\n",
        "\n",
        "cvae = cvae.to(device)\n",
        "optimizer = optim.Adam(cvae.parameters())\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    cvae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        x, y = data\n",
        "        x = x.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, log_var = cvae(x, y)\n",
        "        loss = loss_function(recon_batch, x, mu, log_var)\n",
        "        \n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "    \n",
        "    \n",
        "# Training of CVAE\n",
        "for epoch in range(1, 15):\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ergP_mAuLC9L"
      },
      "source": [
        "### Sanity Check CVAE\n",
        "Check whether your CVAE is able to reconstruct certain images when conditioned on the label (simply run the code).\n",
        "* Is there a difference to the standard VAE?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT8Ug4YBLC9M"
      },
      "outputs": [],
      "source": [
        "x,l = next(iter(train_loader))\n",
        "for i in range(0, 10):\n",
        "    x_one_label = x[l==i][:2]\n",
        "\n",
        "    samples = x_one_label[:1].to(device)\n",
        "    labels= i* torch.ones(1).type(torch.long)\n",
        "    plt.subplot(5,4,2*i+1)\n",
        "    plt.tight_layout()\n",
        "    imshow(samples[0,0].cpu())\n",
        "    plt.title(\"Ori. {}\".format(i))\n",
        "    \n",
        "    samples_rec, _, _ = cvae(samples, c = labels)\n",
        "    samples_rec = samples_rec.detach().cpu().view(-1,28,28)\n",
        "\n",
        "    plt.subplot(5, 4, 2*i+2)\n",
        "    plt.tight_layout()\n",
        "    imshow(samples_rec[0])\n",
        "    plt.title(\"Rec. {}\".format(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MJISGGLLC9M"
      },
      "source": [
        "Check whether your CVAE is able to generate images from the MNIST dataset distribution by sampling from the latent space and decode these latent codes (simply run the code).\n",
        "* How do the generated digits compare to those of the VAE?\n",
        "* Can you imagine why differences could arise?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAardHApLC9M"
      },
      "outputs": [],
      "source": [
        "for i in range(0, 10):\n",
        "    plt.subplot(3,4,i+1)\n",
        "    label = i* torch.ones(1).type(torch.long)\n",
        "    sample = cvae.sampling(n=1, c=label).detach().view(-1,28,28).cpu()\n",
        "    plt.tight_layout()\n",
        "    imshow(sample[0])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"Cond. {}\".format(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjYbcDPNLC9N"
      },
      "source": [
        "## Task 2: Visualisation of Latent Space of VAE\n",
        "\n",
        "\n",
        "### Visualisation of output of decoder\n",
        "Make sure you use the model vae trained with $2$ latent dimensions.\n",
        "\n",
        "* Illustrate the 2 dimensional latent space by showing decoder output for different values in the latent space (see example on exercise sheet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIrnCsjFLC9N"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# TODO #\n",
        "########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikmAOj2ULC9N"
      },
      "source": [
        "### Visualisation of latent space\n",
        "In the following, you should visualize the latent space directly.\n",
        "* Make a scatter plot in latent space, where each plotted point represents the latent code of a single image from the MNIST dataset. Color the points according to the image label.\n",
        "* What kind of shape should ideally arise?\n",
        "* What do you see in reality?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T_mTd-ILC9N"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# TODO #\n",
        "########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3UeDhH_LC9O"
      },
      "source": [
        "### Weaknesses of the VAE\n",
        "* Find from your plot coordinates in the latent space that migh cause problems to the VAE if you decode this points. Explain your reasoning.\n",
        "* Illustrate the decoding of one of these points.\n",
        "* Describe what you see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZtGM8X7LC9O"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# TODO #\n",
        "########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4wl_lPtLC9O"
      },
      "source": [
        "## Task 3: Visualisation of Latent Space of CVAE\n",
        "\n",
        "Make sure you use the model cvae trained with $2$ latent dimensions.\n",
        "\n",
        "### Visualisation of Latent Space via Decoder\n",
        "\n",
        "Repeat task 2 for the CVAE: \n",
        "* Illustrate the 2 dimensional latent space by showing the output of the decoder for different values in the latent space (see example on exercise sheet).\n",
        "* Make two or three of these plots, each conditioned on a fixed label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr3ynRKCLC9P"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# TODO #\n",
        "########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPbFE0AeLC9P"
      },
      "source": [
        "### Visualisation of Latent Space via Decoder\n",
        "* Repeat the scatter plot from Task 2. For each sample, use the correct label as the condition. Color the points according to the label.\n",
        "* What difference do you see, compared to the standard VAE?\n",
        "* How to you explain this?\n",
        "* What does this mean for 'bad samples', as observed in Task 2?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFxPI4mcLC9P"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# TODO #\n",
        "########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzKEGIxLC9P"
      },
      "source": [
        "## Task 4: Generative Classifier\n",
        "We define our classifier as maximum a posteriori estimator and expand according to Bayes rule. The label $\\hat y$ for a given ..$x$ is predicted according to:\n",
        "\n",
        "$$ \\hat y= \\arg \\max_y p(y \\mid x) = \\arg \\max_y \\frac{p(x \\mid y)p(y)}{p(x)} = \\arg \\max_y \\log p(x \\mid y) \\ ,$$\n",
        "\n",
        "where the last identity makes use of the fact that $p(y)=1/10$ is constant for all MNIST labels. We can approximate $\\log p(x \\mid y)$ in the following way: Given an input image, run the CVAE 10 times, each time conditioning one one of the different class labels $y$. Calculate the losses for each case and use them to design a classifier. Note that the network was never trained as classifier, but is still able to perform the task.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV7J1Ry1LC9Q"
      },
      "outputs": [],
      "source": [
        "########\n",
        "# TODO #\n",
        "########"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "cvae.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}